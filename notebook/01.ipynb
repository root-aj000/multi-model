{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71869ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import traceback\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.fg_mfn import FG_MFN, ATTRIBUTE_NAMES\n",
    "from train.logger import Logger\n",
    "from utils.path import TRAIN_CSV, VAL_CSV, SAVED_MODEL_DIR, MODEL_CONFIG\n",
    "from preprocessing.dataset import CustomDataset\n",
    "\n",
    "# ============================================================================\n",
    "# SUPPRESS WARNINGS AND SETUP LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=info, 2=warning, 3=error\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN custom ops\n",
    "\n",
    "# Suppress torchvision warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Configure logging to only show ERROR level and above for specific libraries\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('tensorboard').setLevel(logging.ERROR)\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "logging.getLogger('torchvision').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOGGING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Configure logging for detailed debugging and monitoring\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('training.log'),  # Save logs to file\n",
    "        logging.StreamHandler(sys.stdout)      # Print logs to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# These are the hyperparameters that control the training process\n",
    "# Adjust these based on your dataset size and computational resources\n",
    "\n",
    "BATCH_SIZE = 32  # Number of samples processed together\n",
    "                 # Larger = faster but needs more memory\n",
    "                 # Smaller = slower but more stable gradients\n",
    "\n",
    "EPOCHS = 50  # Maximum number of complete passes through the dataset\n",
    "             # Training may stop earlier due to early stopping\n",
    "\n",
    "LEARNING_RATE = 1e-4  # Step size for weight updates\n",
    "                      # Too high = unstable training\n",
    "                      # Too low = very slow learning\n",
    "\n",
    "WEIGHT_DECAY = 1e-5  # L2 regularization strength\n",
    "                     # Helps prevent overfitting\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 5  # Number of epochs to wait for improvement\n",
    "                             # before stopping training\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
    "\n",
    "RANDOM_SEED = 42  # For reproducible results\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# This ensures you get the same results when running multiple times\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(\"TRAINING CONFIGURATION\")\n",
    "logger.info(\"=\" * 80)\n",
    "logger.info(f\"Device: {DEVICE}\")\n",
    "logger.info(f\"Batch Size: {BATCH_SIZE}\")\n",
    "logger.info(f\"Epochs: {EPOCHS}\")\n",
    "logger.info(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "logger.info(f\"Weight Decay: {WEIGHT_DECAY}\")\n",
    "logger.info(f\"Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "logger.info(f\"Random Seed: {RANDOM_SEED}\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# INITIALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_directories() -> None:\n",
    "    \"\"\"\n",
    "    Create necessary directories for saving models and logs.\n",
    "    \n",
    "    This function ensures all required directories exist before training starts.\n",
    "    If directories don't exist, they will be created.\n",
    "    \n",
    "    Raises:\n",
    "        OSError: If directories cannot be created\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory for saved models\n",
    "        os.makedirs(SAVED_MODEL_DIR, exist_ok=True)\n",
    "        logger.info(f\"âœ“ Model directory: {SAVED_MODEL_DIR}\")\n",
    "        \n",
    "        # Create directory for logs\n",
    "        log_dir = os.path.join(SAVED_MODEL_DIR, \"logs\")\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        logger.info(f\"âœ“ Log directory: {log_dir}\")\n",
    "        \n",
    "        return log_dir\n",
    "        \n",
    "    except OSError as e:\n",
    "        error_msg = f\"Failed to create directories: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        raise OSError(error_msg)\n",
    "\n",
    "\n",
    "def load_model_config() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load model configuration from JSON file.\n",
    "    \n",
    "    The configuration file contains model architecture settings like\n",
    "    backbone types, hidden dimensions, fusion strategies, etc.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model configuration dictionary\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If config file doesn't exist\n",
    "        json.JSONDecodeError: If config file is not valid JSON\n",
    "        ValueError: If config is missing required fields\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading model configuration from: {MODEL_CONFIG}\")\n",
    "        \n",
    "        # Check if config file exists\n",
    "        if not os.path.exists(MODEL_CONFIG):\n",
    "            error_msg = f\"Model config file not found: {MODEL_CONFIG}\"\n",
    "            logger.error(error_msg)\n",
    "            raise FileNotFoundError(error_msg)\n",
    "        \n",
    "        # Load JSON configuration\n",
    "        with open(MODEL_CONFIG, \"r\") as f:\n",
    "            cfg = json.load(f)\n",
    "        \n",
    "        # Validate configuration has required fields\n",
    "        required_fields = [\"IMAGE_BACKBONE\", \"TEXT_ENCODER\", \"HIDDEN_DIM\", \"DROPOUT\"]\n",
    "        missing_fields = [field for field in required_fields if field not in cfg]\n",
    "        \n",
    "        if missing_fields:\n",
    "            error_msg = f\"Config missing required fields: {missing_fields}\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        logger.info(\"âœ“ Model configuration loaded successfully\")\n",
    "        logger.info(f\"  - Image Backbone: {cfg.get('IMAGE_BACKBONE')}\")\n",
    "        logger.info(f\"  - Text Encoder: {cfg.get('TEXT_ENCODER')}\")\n",
    "        logger.info(f\"  - Hidden Dimension: {cfg.get('HIDDEN_DIM')}\")\n",
    "        logger.info(f\"  - Fusion Type: {cfg.get('FUSION_TYPE', 'concat')}\")\n",
    "        \n",
    "        return cfg\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        error_msg = f\"Invalid JSON in config file: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error loading config: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        raise\n",
    "\n",
    "\n",
    "def load_datasets() -> Tuple[CustomDataset, CustomDataset, bool, list]:\n",
    "    \"\"\"\n",
    "    Load training and validation datasets.\n",
    "    \n",
    "    This function creates dataset objects for both training and validation.\n",
    "    It also determines whether we're in legacy mode (single label) or\n",
    "    multi-attribute mode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset, legacy_mode, available_attributes)\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If CSV files don't exist\n",
    "        ValueError: If datasets are empty\n",
    "        RuntimeError: If dataset loading fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Loading datasets...\")\n",
    "        \n",
    "        # Check if CSV files exist\n",
    "        if not os.path.exists(TRAIN_CSV):\n",
    "            error_msg = f\"Training CSV not found: {TRAIN_CSV}\"\n",
    "            logger.error(error_msg)\n",
    "            raise FileNotFoundError(error_msg)\n",
    "        \n",
    "        if not os.path.exists(VAL_CSV):\n",
    "            error_msg = f\"Validation CSV not found: {VAL_CSV}\"\n",
    "            logger.error(error_msg)\n",
    "            raise FileNotFoundError(error_msg)\n",
    "        \n",
    "        # Load training dataset\n",
    "        logger.info(f\"Loading training data from: {TRAIN_CSV}\")\n",
    "        train_dataset = CustomDataset(TRAIN_CSV)\n",
    "        logger.info(f\"âœ“ Training samples: {len(train_dataset)}\")\n",
    "        \n",
    "        # Load validation dataset\n",
    "        logger.info(f\"Loading validation data from: {VAL_CSV}\")\n",
    "        val_dataset = CustomDataset(VAL_CSV)\n",
    "        logger.info(f\"âœ“ Validation samples: {len(val_dataset)}\")\n",
    "        \n",
    "        # Validate dataset sizes\n",
    "        if len(train_dataset) == 0:\n",
    "            error_msg = \"Training dataset is empty\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        if len(val_dataset) == 0:\n",
    "            error_msg = \"Validation dataset is empty\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        # Determine training mode (legacy vs multi-attribute)\n",
    "        legacy_mode = getattr(train_dataset, 'legacy_mode', True)\n",
    "        available_attributes = getattr(train_dataset, 'available_attributes', [])\n",
    "        \n",
    "        logger.info(f\"Training mode: {'Legacy (single label)' if legacy_mode else 'Multi-attribute'}\")\n",
    "        \n",
    "        if not legacy_mode:\n",
    "            logger.info(f\"Available attributes: {available_attributes}\")\n",
    "            if not available_attributes:\n",
    "                logger.warning(\"Multi-attribute mode but no attributes found!\")\n",
    "        \n",
    "        return train_dataset, val_dataset, legacy_mode, available_attributes\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to load datasets: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "def create_data_loaders(\n",
    "    train_dataset: CustomDataset,\n",
    "    val_dataset: CustomDataset,\n",
    "    batch_size: int\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create data loaders for training and validation.\n",
    "    \n",
    "    DataLoaders handle batching, shuffling, and parallel data loading.\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: Training dataset\n",
    "        val_dataset: Validation dataset\n",
    "        batch_size: Number of samples per batch\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader)\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If batch_size is invalid\n",
    "        RuntimeError: If data loader creation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate batch size\n",
    "        if batch_size <= 0:\n",
    "            error_msg = f\"Batch size must be positive, got {batch_size}\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        if batch_size > len(train_dataset):\n",
    "            logger.warning(f\"Batch size ({batch_size}) is larger than training dataset ({len(train_dataset)})\")\n",
    "            logger.warning(f\"Reducing batch size to {len(train_dataset)}\")\n",
    "            batch_size = len(train_dataset)\n",
    "        \n",
    "        logger.info(f\"Creating data loaders with batch size: {batch_size}\")\n",
    "        \n",
    "        # Create training data loader\n",
    "        # shuffle=True: Randomize sample order each epoch (helps generalization)\n",
    "        # num_workers=4: Use 4 parallel processes for data loading (faster)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True if DEVICE == \"cuda\" else False  # Faster GPU transfer\n",
    "        )\n",
    "        \n",
    "        # Create validation data loader\n",
    "        # shuffle=False: Keep order consistent for reproducible validation\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True if DEVICE == \"cuda\" else False\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"âœ“ Training batches: {len(train_loader)}\")\n",
    "        logger.info(f\"âœ“ Validation batches: {len(val_loader)}\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to create data loaders: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "def initialize_model(cfg: Dict[str, Any], device: str) -> FG_MFN:\n",
    "    \"\"\"\n",
    "    Initialize the FG_MFN model and move it to the specified device.\n",
    "    \n",
    "    Args:\n",
    "        cfg: Model configuration dictionary\n",
    "        device: Device to use ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        FG_MFN: Initialized model\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If model initialization fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Initializing FG_MFN model...\")\n",
    "        \n",
    "        # Create model\n",
    "        model = FG_MFN(cfg)\n",
    "        \n",
    "        # Move model to device (GPU or CPU)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        logger.info(\"âœ“ Model initialized successfully\")\n",
    "        logger.info(f\"  - Total parameters: {total_params:,}\")\n",
    "        logger.info(f\"  - Trainable parameters: {trainable_params:,}\")\n",
    "        logger.info(f\"  - Model device: {device}\")\n",
    "        logger.info(f\"  - Number of attribute heads: {len(model.attribute_heads)}\")\n",
    "        logger.info(f\"  - Attributes: {list(model.attribute_heads.keys())}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to initialize model: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "def setup_training_components(\n",
    "    model: nn.Module,\n",
    "    learning_rate: float,\n",
    "    weight_decay: float\n",
    ") -> Tuple[nn.Module, optim.Optimizer, optim.lr_scheduler._LRScheduler]:\n",
    "    \"\"\"\n",
    "    Set up loss function, optimizer, and learning rate scheduler.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        learning_rate: Initial learning rate\n",
    "        weight_decay: L2 regularization coefficient\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (criterion, optimizer, scheduler)\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If parameters are invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate parameters\n",
    "        if learning_rate <= 0:\n",
    "            error_msg = f\"Learning rate must be positive, got {learning_rate}\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        if weight_decay < 0:\n",
    "            error_msg = f\"Weight decay must be non-negative, got {weight_decay}\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "        \n",
    "        logger.info(\"Setting up training components...\")\n",
    "        \n",
    "        # Loss function: CrossEntropyLoss for classification\n",
    "        # Combines LogSoftmax and NLLLoss\n",
    "        # Expects raw logits (not probabilities) as input\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        logger.info(\"âœ“ Loss function: CrossEntropyLoss\")\n",
    "        \n",
    "        # Optimizer: AdamW (Adam with weight decay)\n",
    "        # AdamW is better than Adam for most tasks\n",
    "        # It properly decouples weight decay from gradient updates\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        logger.info(f\"âœ“ Optimizer: AdamW (lr={learning_rate}, weight_decay={weight_decay})\")\n",
    "        \n",
    "        # Learning rate scheduler: ReduceLROnPlateau\n",
    "        # Automatically reduces learning rate when validation loss stops improving\n",
    "        # This helps fine-tune the model in later epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',      # Minimize validation loss\n",
    "            patience=3,      # Wait 3 epochs before reducing\n",
    "            factor=0.5,      # Multiply LR by 0.5 when reducing\n",
    "            verbose=True     # Print when LR is reduced\n",
    "        )\n",
    "        logger.info(\"âœ“ Scheduler: ReduceLROnPlateau (patience=3, factor=0.5)\")\n",
    "        \n",
    "        return criterion, optimizer, scheduler\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to setup training components: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        raise\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: str,\n",
    "    scaler: Optional[torch.cuda.amp.GradScaler] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    This function performs one complete pass through the training dataset,\n",
    "    updating model weights based on the computed loss.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        loader: DataLoader for training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer for weight updates\n",
    "        device: Device to use ('cuda' or 'cpu')\n",
    "        scaler: Gradient scaler for mixed precision training (optional)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of training metrics\n",
    "              Format: {\n",
    "                  'loss': average_loss,\n",
    "                  'attribute_acc': accuracy_for_attribute,\n",
    "                  'attribute_f1': f1_score_for_attribute,\n",
    "                  ...\n",
    "              }\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If training fails\n",
    "    \n",
    "    Note:\n",
    "        This function expects the model to return a dictionary of outputs,\n",
    "        one for each attribute being predicted.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set model to training mode\n",
    "        # This enables dropout and batch normalization in training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Lists to accumulate losses and predictions\n",
    "        train_losses = []\n",
    "        \n",
    "        # Dictionary to store predictions and labels for each attribute\n",
    "        all_preds = {attr: [] for attr in ATTRIBUTE_NAMES}\n",
    "        all_labels = {attr: [] for attr in ATTRIBUTE_NAMES}\n",
    "        \n",
    "        # Progress bar for visual feedback\n",
    "        progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                # Step 1: Move data to device (GPU/CPU)\n",
    "                # batch is a dictionary with keys: 'visual', 'text', 'attention_mask', labels\n",
    "                images = batch[\"visual\"].to(device)\n",
    "                texts = batch[\"text\"].to(device)\n",
    "                masks = batch[\"attention_mask\"].to(device)\n",
    "                \n",
    "                # Step 2: Zero gradients from previous iteration\n",
    "                # PyTorch accumulates gradients by default, so we need to clear them\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Step 3: Forward pass with mixed precision (if using GPU)\n",
    "                # Mixed precision uses float16 for faster computation\n",
    "                # and float32 for numerical stability where needed\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=(scaler is not None)):\n",
    "                    # Get model predictions\n",
    "                    # outputs is a dict: {attr_name: logits_tensor}\n",
    "                    outputs = model(images, texts, attention_mask=masks)\n",
    "                    \n",
    "                    # Step 4: Compute loss\n",
    "                    # We need to handle both legacy mode (single label) and\n",
    "                    # multi-attribute mode differently\n",
    "                    \n",
    "                    # Determine which mode we're in based on the batch\n",
    "                    legacy_mode = \"label\" in batch and len(batch.get(\"label\", [])) > 0\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    num_attrs = 0\n",
    "                    \n",
    "                    if legacy_mode:\n",
    "                        # Legacy mode: single sentiment classification\n",
    "                        # Labels are expected to be 1, 2, 3 (convert to 0, 1, 2)\n",
    "                        labels = (batch[\"label\"] - 1).to(device)\n",
    "                        \n",
    "                        # Use sentiment head if available, otherwise use first head\n",
    "                        if \"sentiment\" in outputs:\n",
    "                            loss = criterion(outputs[\"sentiment\"], labels)\n",
    "                        else:\n",
    "                            # Fallback to first available head\n",
    "                            first_key = list(outputs.keys())[0]\n",
    "                            loss = criterion(outputs[first_key], labels)\n",
    "                        \n",
    "                        total_loss = loss\n",
    "                        num_attrs = 1\n",
    "                        \n",
    "                    else:\n",
    "                        # Multi-attribute mode: compute loss for each attribute\n",
    "                        for attr in ATTRIBUTE_NAMES:\n",
    "                            # Check if this attribute has both predictions and labels\n",
    "                            if attr in outputs and attr in batch:\n",
    "                                # Get labels for this attribute\n",
    "                                labels = batch[attr].to(device)\n",
    "                                \n",
    "                                # Compute loss for this attribute\n",
    "                                loss = criterion(outputs[attr], labels)\n",
    "                                total_loss += loss\n",
    "                                num_attrs += 1\n",
    "                                \n",
    "                                # Track predictions for metric calculation\n",
    "                                preds = torch.argmax(outputs[attr], dim=1).cpu().numpy()\n",
    "                                all_preds[attr].extend(preds)\n",
    "                                all_labels[attr].extend(labels.cpu().numpy())\n",
    "                        \n",
    "                        # Average the loss across all attributes\n",
    "                        # This gives equal weight to each attribute\n",
    "                        if num_attrs > 0:\n",
    "                            total_loss = total_loss / num_attrs\n",
    "                        else:\n",
    "                            # No valid attributes found\n",
    "                            logger.warning(f\"Batch {batch_idx}: No valid attributes for loss computation\")\n",
    "                            continue\n",
    "                \n",
    "                # Step 5: Backward pass\n",
    "                # Compute gradients with respect to model parameters\n",
    "                if scaler is not None:\n",
    "                    # Mixed precision: scale loss to prevent underflow\n",
    "                    scaler.scale(total_loss).backward()\n",
    "                    # Update weights with scaled gradients\n",
    "                    scaler.step(optimizer)\n",
    "                    # Update scaler for next iteration\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    # Standard precision\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Step 6: Record loss\n",
    "                train_losses.append(total_loss.item())\n",
    "                \n",
    "                # Update progress bar with current loss\n",
    "                progress_bar.set_postfix({'loss': total_loss.item()})\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error in training batch {batch_idx}: {str(e)}\"\n",
    "                logger.error(error_msg)\n",
    "                logger.error(traceback.format_exc())\n",
    "                # Continue with next batch instead of stopping entirely\n",
    "                continue\n",
    "        \n",
    "        # Step 7: Compute epoch-level metrics\n",
    "        # Calculate average loss over all batches\n",
    "        if not train_losses:\n",
    "            error_msg = \"No valid training batches processed\"\n",
    "            logger.error(error_msg)\n",
    "            raise RuntimeError(error_msg)\n",
    "        \n",
    "        avg_loss = np.mean(train_losses)\n",
    "        \n",
    "        # Initialize metrics dictionary with loss\n",
    "        metrics = {\"loss\": avg_loss}\n",
    "        \n",
    "        # Step 8: Compute per-attribute metrics\n",
    "        # Calculate accuracy and F1 score for each attribute\n",
    "        for attr in ATTRIBUTE_NAMES:\n",
    "            if all_preds[attr] and all_labels[attr]:\n",
    "                try:\n",
    "                    # Accuracy: percentage of correct predictions\n",
    "                    acc = accuracy_score(all_labels[attr], all_preds[attr])\n",
    "                    metrics[f\"{attr}_acc\"] = acc\n",
    "                    \n",
    "                    # F1 score: harmonic mean of precision and recall\n",
    "                    # weighted: accounts for class imbalance\n",
    "                    f1 = f1_score(\n",
    "                        all_labels[attr],\n",
    "                        all_preds[attr],\n",
    "                        average='weighted',\n",
    "                        zero_division=0  # Return 0 if no predictions\n",
    "                    )\n",
    "                    metrics[f\"{attr}_f1\"] = f1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to compute metrics for {attr}: {str(e)}\")\n",
    "                    # Continue with other attributes\n",
    "                    continue\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Training epoch failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "def validate_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: str\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Validate the model for one epoch.\n",
    "    \n",
    "    This function evaluates the model on the validation dataset\n",
    "    without updating weights. Used to monitor overfitting and\n",
    "    guide early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to validate\n",
    "        loader: DataLoader for validation data\n",
    "        criterion: Loss function\n",
    "        device: Device to use ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of validation metrics\n",
    "              Format same as train_epoch()\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If validation fails\n",
    "    \n",
    "    Note:\n",
    "        This function runs with torch.no_grad() to save memory\n",
    "        and computation since we don't need gradients.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set model to evaluation mode\n",
    "        # This disables dropout and sets batch normalization to use\n",
    "        # running statistics instead of batch statistics\n",
    "        model.eval()\n",
    "        \n",
    "        # Lists to accumulate losses and predictions\n",
    "        val_losses = []\n",
    "        \n",
    "        # Dictionary to store predictions and labels for each attribute\n",
    "        all_preds = {attr: [] for attr in ATTRIBUTE_NAMES}\n",
    "        all_labels = {attr: [] for attr in ATTRIBUTE_NAMES}\n",
    "        \n",
    "        # Progress bar for visual feedback\n",
    "        progress_bar = tqdm(loader, desc=\"Validation\", leave=False)\n",
    "        \n",
    "        # Disable gradient computation\n",
    "        # This saves memory and speeds up computation\n",
    "        # since we don't need gradients during validation\n",
    "        with torch.no_grad():\n",
    "            # Iterate through batches\n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                try:\n",
    "                    # Step 1: Move data to device\n",
    "                    images = batch[\"visual\"].to(device)\n",
    "                    texts = batch[\"text\"].to(device)\n",
    "                    masks = batch[\"attention_mask\"].to(device)\n",
    "                    \n",
    "                    # Step 2: Forward pass (no mixed precision needed for validation)\n",
    "                    outputs = model(images, texts, attention_mask=masks)\n",
    "                    \n",
    "                    # Step 3: Compute loss\n",
    "                    # Same logic as training, but without backward pass\n",
    "                    \n",
    "                    # Determine mode\n",
    "                    legacy_mode = \"label\" in batch and len(batch.get(\"label\", [])) > 0\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    num_attrs = 0\n",
    "                    \n",
    "                    if legacy_mode:\n",
    "                        # Legacy mode: single label\n",
    "                        labels = (batch[\"label\"] - 1).to(device)\n",
    "                        \n",
    "                        if \"sentiment\" in outputs:\n",
    "                            loss = criterion(outputs[\"sentiment\"], labels)\n",
    "                        else:\n",
    "                            first_key = list(outputs.keys())[0]\n",
    "                            loss = criterion(outputs[first_key], labels)\n",
    "                        \n",
    "                        total_loss = loss\n",
    "                        num_attrs = 1\n",
    "                        \n",
    "                    else:\n",
    "                        # Multi-attribute mode\n",
    "                        for attr in ATTRIBUTE_NAMES:\n",
    "                            if attr in outputs and attr in batch:\n",
    "                                labels = batch[attr].to(device)\n",
    "                                loss = criterion(outputs[attr], labels)\n",
    "                                total_loss += loss\n",
    "                                num_attrs += 1\n",
    "                                \n",
    "                                # Track predictions\n",
    "                                preds = torch.argmax(outputs[attr], dim=1).cpu().numpy()\n",
    "                                all_preds[attr].extend(preds)\n",
    "                                all_labels[attr].extend(labels.cpu().numpy())\n",
    "                        \n",
    "                        # Average loss\n",
    "                        if num_attrs > 0:\n",
    "                            total_loss = total_loss / num_attrs\n",
    "                        else:\n",
    "                            logger.warning(f\"Validation batch {batch_idx}: No valid attributes\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Step 4: Record loss\n",
    "                    val_losses.append(total_loss.item())\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({'loss': total_loss.item()})\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error in validation batch {batch_idx}: {str(e)}\"\n",
    "                    logger.error(error_msg)\n",
    "                    # Continue with next batch\n",
    "                    continue\n",
    "        \n",
    "        # Step 5: Compute epoch-level metrics\n",
    "        if not val_losses:\n",
    "            error_msg = \"No valid validation batches processed\"\n",
    "            logger.error(error_msg)\n",
    "            raise RuntimeError(error_msg)\n",
    "        \n",
    "        avg_loss = np.mean(val_losses)\n",
    "        \n",
    "        # Initialize metrics dictionary\n",
    "        metrics = {\"loss\": avg_loss}\n",
    "        \n",
    "        # Step 6: Compute per-attribute metrics\n",
    "        for attr in ATTRIBUTE_NAMES:\n",
    "            if all_preds[attr] and all_labels[attr]:\n",
    "                try:\n",
    "                    # Accuracy\n",
    "                    acc = accuracy_score(all_labels[attr], all_preds[attr])\n",
    "                    metrics[f\"{attr}_acc\"] = acc\n",
    "                    \n",
    "                    # F1 score\n",
    "                    f1 = f1_score(\n",
    "                        all_labels[attr],\n",
    "                        all_preds[attr],\n",
    "                        average='weighted',\n",
    "                        zero_division=0\n",
    "                    )\n",
    "                    metrics[f\"{attr}_f1\"] = f1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to compute validation metrics for {attr}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Validation epoch failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training function.\n",
    "    \n",
    "    This is the entry point for the training script. It:\n",
    "    1. Sets up all necessary components (data, model, optimizer, etc.)\n",
    "    2. Runs the training loop for multiple epochs\n",
    "    3. Validates after each epoch\n",
    "    4. Saves checkpoints\n",
    "    5. Implements early stopping\n",
    "    \n",
    "    The function handles all error cases and ensures proper cleanup.\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: If training setup or execution fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Record training start time\n",
    "        start_time = datetime.now()\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"STARTING TRAINING\")\n",
    "        logger.info(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 1: SETUP - Initialize all components\n",
    "        # =====================================================================\n",
    "        \n",
    "        logger.info(\"\\n[STEP 1/6] Setting up training environment...\")\n",
    "        \n",
    "        # Create necessary directories\n",
    "        log_dir = setup_directories()\n",
    "        \n",
    "        # Load model configuration\n",
    "        cfg = load_model_config()\n",
    "        \n",
    "        # Load datasets\n",
    "        train_dataset, val_dataset, legacy_mode, available_attributes = load_datasets()\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader, val_loader = create_data_loaders(\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = initialize_model(cfg, DEVICE)\n",
    "        \n",
    "        # Setup training components\n",
    "        criterion, optimizer, scheduler = setup_training_components(\n",
    "            model,\n",
    "            LEARNING_RATE,\n",
    "            WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # Initialize logger for metrics\n",
    "        metrics_logger = Logger(log_dir)\n",
    "        logger.info(\"âœ“ Metrics logger initialized\")\n",
    "        \n",
    "        # Setup mixed precision training (if using GPU)\n",
    "        scaler = None\n",
    "        if DEVICE == \"cuda\":\n",
    "            scaler = torch.cuda.amp.GradScaler(\"cuda\")\n",
    "            logger.info(\"âœ“ Mixed precision training enabled\")\n",
    "        else:\n",
    "            logger.info(\"â„¹ Mixed precision disabled (CPU mode)\")\n",
    "        \n",
    "        logger.info(\"\\nâœ“ Setup complete!\\n\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 2: TRAINING PREPARATION\n",
    "        # =====================================================================\n",
    "        \n",
    "        logger.info(\"[STEP 2/6] Preparing for training...\")\n",
    "        \n",
    "        # Early stopping variables\n",
    "        best_val_loss = float('inf')  # Initialize to infinity\n",
    "        patience_counter = 0           # Count epochs without improvement\n",
    "        \n",
    "        # Training statistics\n",
    "        training_history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"âœ“ Early stopping patience: {EARLY_STOPPING_PATIENCE} epochs\")\n",
    "        logger.info(f\"âœ“ Best model will be saved based on validation loss\")\n",
    "        logger.info(\"\\n\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 3: TRAINING LOOP\n",
    "        # =====================================================================\n",
    "        \n",
    "        logger.info(\"[STEP 3/6] Starting training loop...\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            try:\n",
    "                logger.info(f\"\\nEPOCH {epoch}/{EPOCHS}\")\n",
    "                logger.info(\"-\" * 80)\n",
    "                \n",
    "                # Get current learning rate\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                logger.info(f\"Learning Rate: {current_lr:.6f}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 3A: TRAINING PHASE\n",
    "                # ============================================================\n",
    "                \n",
    "                logger.info(\"\\nTraining phase...\")\n",
    "                train_metrics = train_epoch(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    DEVICE,\n",
    "                    scaler\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"âœ“ Training complete - Loss: {train_metrics['loss']:.4f}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 3B: VALIDATION PHASE\n",
    "                # ============================================================\n",
    "                \n",
    "                logger.info(\"\\nValidation phase...\")\n",
    "                val_metrics = validate_epoch(\n",
    "                    model,\n",
    "                    val_loader,\n",
    "                    criterion,\n",
    "                    DEVICE\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"âœ“ Validation complete - Loss: {val_metrics['loss']:.4f}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 3C: LOG METRICS\n",
    "                # ============================================================\n",
    "                \n",
    "                # Combine all metrics for logging\n",
    "                log_data = {}\n",
    "                \n",
    "                # Add training metrics with 'train_' prefix\n",
    "                for key, value in train_metrics.items():\n",
    "                    log_data[f\"train_{key}\"] = value\n",
    "                \n",
    "                # Add validation metrics with 'val_' prefix\n",
    "                for key, value in val_metrics.items():\n",
    "                    log_data[f\"val_{key}\"] = value\n",
    "                \n",
    "                # Add learning rate\n",
    "                log_data['learning_rate'] = current_lr\n",
    "                \n",
    "                # Log to file\n",
    "                metrics_logger.log_metrics(log_data, epoch)\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 3D: PRINT SUMMARY\n",
    "                # ============================================================\n",
    "                \n",
    "                train_loss = train_metrics[\"loss\"]\n",
    "                val_loss = val_metrics[\"loss\"]\n",
    "                \n",
    "                logger.info(\"\\n\" + \"=\" * 80)\n",
    "                logger.info(f\"EPOCH {epoch} SUMMARY\")\n",
    "                logger.info(\"=\" * 80)\n",
    "                logger.info(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "                \n",
    "                # Print per-attribute metrics if available\n",
    "                logger.info(\"\\nPer-Attribute Performance:\")\n",
    "                for attr in ATTRIBUTE_NAMES:\n",
    "                    train_acc_key = f\"{attr}_acc\"\n",
    "                    val_acc_key = f\"{attr}_acc\"\n",
    "                    \n",
    "                    if train_acc_key in train_metrics:\n",
    "                        train_acc = train_metrics[train_acc_key]\n",
    "                        val_acc = val_metrics.get(val_acc_key, 0.0)\n",
    "                        \n",
    "                        train_f1 = train_metrics.get(f\"{attr}_f1\", 0.0)\n",
    "                        val_f1 = val_metrics.get(f\"{attr}_f1\", 0.0)\n",
    "                        \n",
    "                        logger.info(f\"  {attr:20s}: Train Acc={train_acc:.3f} | Val Acc={val_acc:.3f} | \"\n",
    "                                  f\"Train F1={train_f1:.3f} | Val F1={val_f1:.3f}\")\n",
    "                \n",
    "                logger.info(\"=\" * 80)\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 3E: LEARNING RATE SCHEDULING\n",
    "                # ============================================================\n",
    "                \n",
    "                # Update learning rate based on validation loss\n",
    "                # The scheduler will reduce LR if val_loss doesn't improve\n",
    "                old_lr = optimizer.param_groups[0]['lr']\n",
    "                scheduler.step(val_loss)\n",
    "                new_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                if new_lr != old_lr:\n",
    "                    logger.info(f\"\\nðŸ“‰ Learning rate reduced: {old_lr:.6f} â†’ {new_lr:.6f}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 3F: SAVE CHECKPOINTS\n",
    "                # ============================================================\n",
    "                \n",
    "                # Always save the latest model\n",
    "                last_model_path = os.path.join(SAVED_MODEL_DIR, \"model_last.pt\")\n",
    "                try:\n",
    "                    torch.save(model.state_dict(), last_model_path)\n",
    "                    logger.info(f\"\\nðŸ’¾ Saved latest model: {last_model_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to save last model: {str(e)}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 3G: EARLY STOPPING CHECK\n",
    "                # ============================================================\n",
    "                \n",
    "                # Check if validation loss improved\n",
    "                if val_loss < best_val_loss:\n",
    "                    # New best model!\n",
    "                    improvement = best_val_loss - val_loss\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    best_model_path = os.path.join(SAVED_MODEL_DIR, \"model_best.pt\")\n",
    "                    try:\n",
    "                        torch.save(model.state_dict(), best_model_path)\n",
    "                        logger.info(f\"ðŸŒŸ New best model! Validation loss improved by {improvement:.4f}\")\n",
    "                        logger.info(f\"ðŸ’¾ Saved best model: {best_model_path}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Failed to save best model: {str(e)}\")\n",
    "                    \n",
    "                else:\n",
    "                    # No improvement\n",
    "                    patience_counter += 1\n",
    "                    logger.info(f\"\\nâš  No improvement in validation loss for {patience_counter} epoch(s)\")\n",
    "                    logger.info(f\"   Best val loss: {best_val_loss:.4f} | Current: {val_loss:.4f}\")\n",
    "                    \n",
    "                    # Check if we should stop early\n",
    "                    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                        logger.info(\"\\n\" + \"=\" * 80)\n",
    "                        logger.info(f\"ðŸ›‘ EARLY STOPPING at epoch {epoch}\")\n",
    "                        logger.info(f\"   No improvement for {EARLY_STOPPING_PATIENCE} consecutive epochs\")\n",
    "                        logger.info(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "                        logger.info(\"=\" * 80)\n",
    "                        break\n",
    "                \n",
    "                # Update training history\n",
    "                training_history['train_loss'].append(train_loss)\n",
    "                training_history['val_loss'].append(val_loss)\n",
    "                training_history['learning_rates'].append(current_lr)\n",
    "                \n",
    "                logger.info(\"\\n\")  # Add spacing between epochs\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error in epoch {epoch}: {str(e)}\"\n",
    "                logger.error(error_msg)\n",
    "                logger.error(traceback.format_exc())\n",
    "                \n",
    "                # Save emergency checkpoint\n",
    "                emergency_path = os.path.join(SAVED_MODEL_DIR, f\"model_epoch_{epoch}_emergency.pt\")\n",
    "                try:\n",
    "                    torch.save(model.state_dict(), emergency_path)\n",
    "                    logger.info(f\"ðŸ’¾ Saved emergency checkpoint: {emergency_path}\")\n",
    "                except:\n",
    "                    logger.error(\"Failed to save emergency checkpoint\")\n",
    "                \n",
    "                # Decide whether to continue or stop\n",
    "                logger.info(\"Attempting to continue with next epoch...\")\n",
    "                continue\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 4: TRAINING COMPLETE\n",
    "        # =====================================================================\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"TRAINING COMPLETE\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # Calculate training duration\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        logger.info(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        logger.info(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        logger.info(f\"Total duration: {duration}\")\n",
    "        logger.info(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 5: SAVE TRAINING HISTORY\n",
    "        # =====================================================================\n",
    "        \n",
    "        logger.info(\"\\n[STEP 5/6] Saving training history...\")\n",
    "        \n",
    "        try:\n",
    "            history_path = os.path.join(SAVED_MODEL_DIR, \"training_history.json\")\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(training_history, f, indent=2)\n",
    "            logger.info(f\"âœ“ Training history saved: {history_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save training history: {str(e)}\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STEP 6: CLEANUP\n",
    "        # =====================================================================\n",
    "        \n",
    "        logger.info(\"\\n[STEP 6/6] Cleaning up...\")\n",
    "        \n",
    "        try:\n",
    "            # Close metrics logger\n",
    "            metrics_logger.close()\n",
    "            logger.info(\"âœ“ Metrics logger closed\")\n",
    "            \n",
    "            # Clear GPU cache if using CUDA\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                logger.info(\"âœ“ GPU cache cleared\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cleanup warning: {str(e)}\")\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"ALL DONE! ðŸŽ‰\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        # Handle Ctrl+C gracefully\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"âš  Training interrupted by user\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # Try to save current state\n",
    "        try:\n",
    "            interrupt_path = os.path.join(SAVED_MODEL_DIR, \"model_interrupted.pt\")\n",
    "            torch.save(model.state_dict(), interrupt_path)\n",
    "            logger.info(f\"ðŸ’¾ Saved interrupted model: {interrupt_path}\")\n",
    "        except:\n",
    "            logger.error(\"Failed to save interrupted model\")\n",
    "        \n",
    "        sys.exit(0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any unexpected errors\n",
    "        logger.error(\"\\n\" + \"=\" * 80)\n",
    "        logger.error(\"âŒ TRAINING FAILED\")\n",
    "        logger.error(\"=\" * 80)\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        logger.error(\"\\nFull traceback:\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        logger.error(\"=\" * 80)\n",
    "        \n",
    "        # Try to save current state for debugging\n",
    "        try:\n",
    "            error_path = os.path.join(SAVED_MODEL_DIR, \"model_error_state.pt\")\n",
    "            torch.save(model.state_dict(), error_path)\n",
    "            logger.info(f\"ðŸ’¾ Saved error state model: {error_path}\")\n",
    "        except:\n",
    "            logger.error(\"Failed to save error state model\")\n",
    "        \n",
    "        raise\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRY POINT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Script entry point.\n",
    "    \n",
    "    This block only runs when the script is executed directly\n",
    "    (not when imported as a module).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Fatal error: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES AND DOCUMENTATION\n",
    "# =============================================================================\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
